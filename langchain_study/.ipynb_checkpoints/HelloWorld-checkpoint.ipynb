{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.获取大模型",
   "id": "f1dd22d65076c504"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-30T12:02:20.419222Z",
     "start_time": "2025-11-30T12:01:48.779729Z"
    }
   },
   "source": [
    "from tabnanny import verbose\n",
    "\n",
    "import dotenv\n",
    "from jedi.settings import cache_directory\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.tracers import LangChainTracer\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "from openai import max_retries\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv('OPENAI_BASE_URL')\n",
    "\n",
    "# 创建大模型实例\n",
    "llm = ChatOpenAI(model=os.getenv('OPENAI_MODEL'))\n",
    "\n",
    "# 直接提供问题，并调用llm\n",
    "response = llm.invoke(\"什么是大模型？\")\n",
    "print(response)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mConnectError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001B[39m, in \u001B[36mHTTPTransport.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     resp = \u001B[38;5;28mself\u001B[39m._pool.handle_request(req)\n\u001B[32m    252\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.Iterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    255\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = connection.handle_request(\n\u001B[32m    237\u001B[39m         pool_request.request\n\u001B[32m    238\u001B[39m     )\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpcore/_sync/http_proxy.py:316\u001B[39m, in \u001B[36mTunnelHTTPConnection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mstart_tls\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     stream = stream.start_tls(**kwargs)\n\u001B[32m    317\u001B[39m     trace.return_value = stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpcore/_sync/http11.py:376\u001B[39m, in \u001B[36mHTTP11UpgradeStream.start_tls\u001B[39m\u001B[34m(self, ssl_context, server_hostname, timeout)\u001B[39m\n\u001B[32m    370\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstart_tls\u001B[39m(\n\u001B[32m    371\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    372\u001B[39m     ssl_context: ssl.SSLContext,\n\u001B[32m    373\u001B[39m     server_hostname: \u001B[38;5;28mstr\u001B[39m | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    374\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    375\u001B[39m ) -> NetworkStream:\n\u001B[32m--> \u001B[39m\u001B[32m376\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stream.start_tls(ssl_context, server_hostname, timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpcore/_backends/sync.py:154\u001B[39m, in \u001B[36mSyncStream.start_tls\u001B[39m\u001B[34m(self, ssl_context, server_hostname, timeout)\u001B[39m\n\u001B[32m    150\u001B[39m exc_map: ExceptionMapping = {\n\u001B[32m    151\u001B[39m     socket.timeout: ConnectTimeout,\n\u001B[32m    152\u001B[39m     \u001B[38;5;167;01mOSError\u001B[39;00m: ConnectError,\n\u001B[32m    153\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    155\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/contextlib.py:158\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m     \u001B[38;5;28mself\u001B[39m.gen.throw(value)\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    160\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    161\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001B[39m, in \u001B[36mmap_exceptions\u001B[39m\u001B[34m(map)\u001B[39m\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mConnectError\u001B[39m: [Errno 54] Connection reset by peer",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mConnectError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/openai/_base_client.py:982\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m    981\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m982\u001B[39m     response = \u001B[38;5;28mself\u001B[39m._client.send(\n\u001B[32m    983\u001B[39m         request,\n\u001B[32m    984\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_stream_response_body(request=request),\n\u001B[32m    985\u001B[39m         **kwargs,\n\u001B[32m    986\u001B[39m     )\n\u001B[32m    987\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m httpx.TimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpx/_client.py:914\u001B[39m, in \u001B[36mClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m    912\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m--> \u001B[39m\u001B[32m914\u001B[39m response = \u001B[38;5;28mself\u001B[39m._send_handling_auth(\n\u001B[32m    915\u001B[39m     request,\n\u001B[32m    916\u001B[39m     auth=auth,\n\u001B[32m    917\u001B[39m     follow_redirects=follow_redirects,\n\u001B[32m    918\u001B[39m     history=[],\n\u001B[32m    919\u001B[39m )\n\u001B[32m    920\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpx/_client.py:942\u001B[39m, in \u001B[36mClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     response = \u001B[38;5;28mself\u001B[39m._send_handling_redirects(\n\u001B[32m    943\u001B[39m         request,\n\u001B[32m    944\u001B[39m         follow_redirects=follow_redirects,\n\u001B[32m    945\u001B[39m         history=history,\n\u001B[32m    946\u001B[39m     )\n\u001B[32m    947\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpx/_client.py:979\u001B[39m, in \u001B[36mClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m    977\u001B[39m     hook(request)\n\u001B[32m--> \u001B[39m\u001B[32m979\u001B[39m response = \u001B[38;5;28mself\u001B[39m._send_single_request(request)\n\u001B[32m    980\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpx/_client.py:1014\u001B[39m, in \u001B[36mClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1013\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m     response = transport.handle_request(request)\n\u001B[32m   1016\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, SyncByteStream)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpx/_transports/default.py:249\u001B[39m, in \u001B[36mHTTPTransport.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    237\u001B[39m req = httpcore.Request(\n\u001B[32m    238\u001B[39m     method=request.method,\n\u001B[32m    239\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    247\u001B[39m     extensions=request.extensions,\n\u001B[32m    248\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m249\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m    250\u001B[39m     resp = \u001B[38;5;28mself\u001B[39m._pool.handle_request(req)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/contextlib.py:158\u001B[39m, in \u001B[36m_GeneratorContextManager.__exit__\u001B[39m\u001B[34m(self, typ, value, traceback)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m     \u001B[38;5;28mself\u001B[39m.gen.throw(value)\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    160\u001B[39m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[32m    161\u001B[39m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001B[39m, in \u001B[36mmap_httpcore_exceptions\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    117\u001B[39m message = \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[31mConnectError\u001B[39m: [Errno 54] Connection reset by peer",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mAPIConnectionError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     10\u001B[39m llm = ChatOpenAI(model=os.getenv(\u001B[33m'\u001B[39m\u001B[33mOPENAI_MODEL\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# 直接提供问题，并调用llm\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m response = llm.invoke(\u001B[33m\"\u001B[39m\u001B[33m什么是大模型？\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:398\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    384\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    385\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    386\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    391\u001B[39m     **kwargs: Any,\n\u001B[32m    392\u001B[39m ) -> AIMessage:\n\u001B[32m    393\u001B[39m     config = ensure_config(config)\n\u001B[32m    394\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    395\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAIMessage\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    396\u001B[39m         cast(\n\u001B[32m    397\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m--> \u001B[39m\u001B[32m398\u001B[39m             \u001B[38;5;28mself\u001B[39m.generate_prompt(\n\u001B[32m    399\u001B[39m                 [\u001B[38;5;28mself\u001B[39m._convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[32m    400\u001B[39m                 stop=stop,\n\u001B[32m    401\u001B[39m                 callbacks=config.get(\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    402\u001B[39m                 tags=config.get(\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    403\u001B[39m                 metadata=config.get(\u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    404\u001B[39m                 run_name=config.get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    405\u001B[39m                 run_id=config.pop(\u001B[33m\"\u001B[39m\u001B[33mrun_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    406\u001B[39m                 **kwargs,\n\u001B[32m    407\u001B[39m             ).generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    408\u001B[39m         ).message,\n\u001B[32m    409\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1117\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m   1108\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   1109\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m   1110\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1114\u001B[39m     **kwargs: Any,\n\u001B[32m   1115\u001B[39m ) -> LLMResult:\n\u001B[32m   1116\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m-> \u001B[39m\u001B[32m1117\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:927\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    924\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    925\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    926\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m927\u001B[39m             \u001B[38;5;28mself\u001B[39m._generate_with_cache(\n\u001B[32m    928\u001B[39m                 m,\n\u001B[32m    929\u001B[39m                 stop=stop,\n\u001B[32m    930\u001B[39m                 run_manager=run_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    931\u001B[39m                 **kwargs,\n\u001B[32m    932\u001B[39m             )\n\u001B[32m    933\u001B[39m         )\n\u001B[32m    934\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    935\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1221\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1219\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1220\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1221\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(\n\u001B[32m   1222\u001B[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001B[32m   1223\u001B[39m     )\n\u001B[32m   1224\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1225\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1356\u001B[39m, in \u001B[36mBaseChatOpenAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1354\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[33m\"\u001B[39m\u001B[33mhttp_response\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   1355\u001B[39m         e.response = raw_response.http_response  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1356\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m   1357\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1358\u001B[39m     \u001B[38;5;28mself\u001B[39m.include_response_headers\n\u001B[32m   1359\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1360\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1361\u001B[39m ):\n\u001B[32m   1362\u001B[39m     generation_info = {\u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response.headers)}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1351\u001B[39m, in \u001B[36mBaseChatOpenAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1344\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m _construct_lc_result_from_responses_api(\n\u001B[32m   1345\u001B[39m             response,\n\u001B[32m   1346\u001B[39m             schema=original_schema_obj,\n\u001B[32m   1347\u001B[39m             metadata=generation_info,\n\u001B[32m   1348\u001B[39m             output_version=\u001B[38;5;28mself\u001B[39m.output_version,\n\u001B[32m   1349\u001B[39m         )\n\u001B[32m   1350\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1351\u001B[39m         raw_response = \u001B[38;5;28mself\u001B[39m.client.with_raw_response.create(**payload)\n\u001B[32m   1352\u001B[39m         response = raw_response.parse()\n\u001B[32m   1353\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001B[39m, in \u001B[36mto_raw_response_wrapper.<locals>.wrapped\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    360\u001B[39m extra_headers[RAW_RESPONSE_HEADER] = \u001B[33m\"\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    362\u001B[39m kwargs[\u001B[33m\"\u001B[39m\u001B[33mextra_headers\u001B[39m\u001B[33m\"\u001B[39m] = extra_headers\n\u001B[32m--> \u001B[39m\u001B[32m364\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    284\u001B[39m             msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[32m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    285\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m286\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1147\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   1101\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m   1102\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m   1103\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1144\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = not_given,\n\u001B[32m   1145\u001B[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[32m   1146\u001B[39m     validate_response_format(response_format)\n\u001B[32m-> \u001B[39m\u001B[32m1147\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._post(\n\u001B[32m   1148\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m/chat/completions\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1149\u001B[39m         body=maybe_transform(\n\u001B[32m   1150\u001B[39m             {\n\u001B[32m   1151\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m: messages,\n\u001B[32m   1152\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model,\n\u001B[32m   1153\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33maudio\u001B[39m\u001B[33m\"\u001B[39m: audio,\n\u001B[32m   1154\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfrequency_penalty\u001B[39m\u001B[33m\"\u001B[39m: frequency_penalty,\n\u001B[32m   1155\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunction_call\u001B[39m\u001B[33m\"\u001B[39m: function_call,\n\u001B[32m   1156\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunctions\u001B[39m\u001B[33m\"\u001B[39m: functions,\n\u001B[32m   1157\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogit_bias\u001B[39m\u001B[33m\"\u001B[39m: logit_bias,\n\u001B[32m   1158\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogprobs\u001B[39m\u001B[33m\"\u001B[39m: logprobs,\n\u001B[32m   1159\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_completion_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_completion_tokens,\n\u001B[32m   1160\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_tokens,\n\u001B[32m   1161\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m   1162\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodalities\u001B[39m\u001B[33m\"\u001B[39m: modalities,\n\u001B[32m   1163\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[33m\"\u001B[39m: n,\n\u001B[32m   1164\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mparallel_tool_calls\u001B[39m\u001B[33m\"\u001B[39m: parallel_tool_calls,\n\u001B[32m   1165\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprediction\u001B[39m\u001B[33m\"\u001B[39m: prediction,\n\u001B[32m   1166\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mpresence_penalty\u001B[39m\u001B[33m\"\u001B[39m: presence_penalty,\n\u001B[32m   1167\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_key\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_key,\n\u001B[32m   1168\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mreasoning_effort\u001B[39m\u001B[33m\"\u001B[39m: reasoning_effort,\n\u001B[32m   1169\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mresponse_format\u001B[39m\u001B[33m\"\u001B[39m: response_format,\n\u001B[32m   1170\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33msafety_identifier\u001B[39m\u001B[33m\"\u001B[39m: safety_identifier,\n\u001B[32m   1171\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mseed\u001B[39m\u001B[33m\"\u001B[39m: seed,\n\u001B[32m   1172\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mservice_tier\u001B[39m\u001B[33m\"\u001B[39m: service_tier,\n\u001B[32m   1173\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m: stop,\n\u001B[32m   1174\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstore\u001B[39m\u001B[33m\"\u001B[39m: store,\n\u001B[32m   1175\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m: stream,\n\u001B[32m   1176\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream_options\u001B[39m\u001B[33m\"\u001B[39m: stream_options,\n\u001B[32m   1177\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m: temperature,\n\u001B[32m   1178\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtool_choice\u001B[39m\u001B[33m\"\u001B[39m: tool_choice,\n\u001B[32m   1179\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m: tools,\n\u001B[32m   1180\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_logprobs\u001B[39m\u001B[33m\"\u001B[39m: top_logprobs,\n\u001B[32m   1181\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_p\u001B[39m\u001B[33m\"\u001B[39m: top_p,\n\u001B[32m   1182\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: user,\n\u001B[32m   1183\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mverbosity\u001B[39m\u001B[33m\"\u001B[39m: verbosity,\n\u001B[32m   1184\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mweb_search_options\u001B[39m\u001B[33m\"\u001B[39m: web_search_options,\n\u001B[32m   1185\u001B[39m             },\n\u001B[32m   1186\u001B[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001B[32m   1187\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m stream\n\u001B[32m   1188\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001B[32m   1189\u001B[39m         ),\n\u001B[32m   1190\u001B[39m         options=make_request_options(\n\u001B[32m   1191\u001B[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001B[32m   1192\u001B[39m         ),\n\u001B[32m   1193\u001B[39m         cast_to=ChatCompletion,\n\u001B[32m   1194\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1195\u001B[39m         stream_cls=Stream[ChatCompletionChunk],\n\u001B[32m   1196\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/openai/_base_client.py:1259\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1245\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1246\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1247\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1254\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1255\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1256\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1257\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1258\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1259\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/ai_agent/lib/python3.12/site-packages/openai/_base_client.py:1014\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1011\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1013\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRaising connection error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(request=request) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   1016\u001B[39m log.debug(\n\u001B[32m   1017\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mHTTP Response: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m'\u001B[39m,\n\u001B[32m   1018\u001B[39m     request.method,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     response.headers,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m log.debug(\u001B[33m\"\u001B[39m\u001B[33mrequest_id: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, response.headers.get(\u001B[33m\"\u001B[39m\u001B[33mx-request-id\u001B[39m\u001B[33m\"\u001B[39m))\n",
      "\u001B[31mAPIConnectionError\u001B[39m: Connection error."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.使用提示词模板\n",
   "id": "7500e605cc342bcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 需要注意的一点是，这里需要指明具体的role，在这里是system和用户\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是世界级的技术文档编写者\"),\n",
    "    (\"user\", \"{input}\")  # {input} 为变量\n",
    "])\n",
    "\n",
    "# 我们可以把prompt和具体的llm的调用合在一起\n",
    "chain= prompt | llm\n",
    "message = chain.invoke({\"input\":\"大模型中的langchain是什么\"})\n",
    "print(message)\n",
    "print(type(message))\n"
   ],
   "id": "c584185a5a29ed10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.使用输出解析器",
   "id": "6e099e2554e0e234"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "#使用输出解析器\n",
    "output_parser = JsonOutputParser()\n",
    "#将其添加到一个链上\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 答案是一个字符串，而不是ChatMessage\n",
    "# chain.invoke({\"input\":\"langchain是什么\"})\n",
    "\n",
    "chain.invoke({\"input\":\"LangChain是什么？用JSON格式回复，问题用question，回答用answer\"})\n"
   ],
   "id": "2f7a3ce6dfc6f24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4.使用向量存储\n",
    "\n",
    "pip install faiss-cpu\n",
    "\n",
    "pip install langchain-community\n"
   ],
   "id": "4946d103ab13ce0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path = \"https://www.gov.cn/xinwen/2020-06/01/countent_5516649.htm\",\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(id=\"UCAP-CONTENT\"))\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 使用分割器分割文档\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "print(documents)\n",
    "# 向量存储 embeddings 会将documents中的每个文本片段转换为向量，并将这些向量存储在 FAISS向量数据库中\n",
    "vector = FAISS.from_documents(documents, embeddings)\n"
   ],
   "id": "9b53e8f5ad9180d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5.RAG(检索增强生成)",
   "id": "83e2b7b35a19a919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retriever.search_kwargs = {\"k\":3}\n",
    "docs = retriever.invoker(\"建设用地使用权是什么？\")\n",
    "\n",
    "# for i,doc in enumerate(docs):\n",
    "#     print(f\"第{i+1}条规则：\")\n",
    "#     print(doc)\n",
    "\n",
    "# 定义提示词模板\n",
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人。\n",
    "你的任务是根据下述给定的已知信息回答用户问题。\n",
    "确保你的回复完全依据下述已知信息。不要编造答案。\n",
    "如果下述已知信息不足以回答用户的问题，请直接回复“我无法回答您的问题”。\n",
    "\n",
    "已知信息:\n",
    "{info}\n",
    "\n",
    "用户问:\n",
    "{question}\n",
    "\n",
    "请用中文回答用户问题。\n",
    "\"\"\"\n",
    "template = PromptTemplate.from_template(prompt_template)\n",
    "prompt = template.format(info=docs, question=\"建设用地使用权是什么？\")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)\n"
   ],
   "id": "33b20d351128c41d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 6.使用Agent"
   ],
   "id": "8460ec7c0662fcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# 检索器工具\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"CivilCodeRetriever\",\n",
    "    \"搜索有关中华人民共和国民法典的信息，关于中华人民共和国民法典的任何问题，您必须使用此工具！\"\n",
    ")\n",
    "\n",
    "#内置工具\n",
    "web_search = TavilySearch.i\n",
    "\n",
    "tools = [retriever_tool, web_search]\n",
    "from langchain_classic import hub\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_agent(model=llm, tools=tools, system_prompt=prompt)\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"message\": [{\"role\": \"user\", \"content\": \"建设用地使用权是什么\"}]\n",
    "})\n",
    "print(result)\n",
    "#\n",
    "print(result['messages'][-1].content)"
   ],
   "id": "aa8dac9eb01dc157"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T13:09:27.578771Z",
     "start_time": "2025-11-30T13:09:27.571931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "l = [1,2,3,4,5]\n",
    "print(l[0])\n",
    "print(l[-2])"
   ],
   "id": "a514192942225cc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
